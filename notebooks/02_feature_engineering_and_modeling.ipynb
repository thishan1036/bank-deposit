{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d1baa9f",
   "metadata": {},
   "source": [
    "## **Boosting Marketing Efficiency: Targeted Bank Campaign by Customer Subscription Behavior**\n",
    "\n",
    "**Overall Project Objective:** \n",
    "\n",
    "Develop a data-driven marketing strategy that maximizes return on investment (ROI) by identifying optimal trade-off between broad customer outreach and precision marketing using predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964941a",
   "metadata": {},
   "source": [
    "**Notebook 2 of 3: Feature Engineering & Predictive Modeling**\n",
    "\n",
    "This notebook covers the core technical steps of the predictive modeling pipeline. \n",
    "\n",
    "The primary goals are to:\n",
    "- Prepare the cleaned data for machine learning through feature engineering.\n",
    "- Build and evaluate several classification models.\n",
    "- Identify the best-performing model for predicting term deposit subscriptions.\n",
    "- Analyze the precision-recall trade-off to inform the final strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e22222c",
   "metadata": {},
   "source": [
    "### **Setup & Data Load**\n",
    "\n",
    "Simple data and libraries load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f1230ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0256b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/bank_cleaned.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e9e946",
   "metadata": {},
   "source": [
    "**-Data Overview-**\n",
    "\n",
    "The specific file used is 'bank_cleaned.csv,' which is the cleaned and imputed output from notebook 1. \n",
    "\n",
    "Data at a glance:\n",
    "- Dataset: bank_cleaned.csv\n",
    "- Observations: 4,521\n",
    "- Variables: 17 (pre-feature engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "024c4561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>1787</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>19</td>\n",
       "      <td>oct</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown_outcome</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>4789</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>11</td>\n",
       "      <td>may</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>339</td>\n",
       "      <td>4</td>\n",
       "      <td>failure</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>1350</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>16</td>\n",
       "      <td>apr</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>330</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>1476</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>3</td>\n",
       "      <td>jun</td>\n",
       "      <td>199</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown_outcome</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown_outcome</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          job  marital  education default  balance housing loan  \\\n",
       "0   30   unemployed  married    primary      no     1787      no   no   \n",
       "1   33     services  married  secondary      no     4789     yes  yes   \n",
       "2   35   management   single   tertiary      no     1350     yes   no   \n",
       "3   30   management  married   tertiary      no     1476     yes  yes   \n",
       "4   59  blue-collar  married  secondary      no        0     yes   no   \n",
       "\n",
       "       contact  day month  duration  campaign  pdays  previous  \\\n",
       "0     cellular   19   oct        79         1     -1         0   \n",
       "1     cellular   11   may       220         1    339         4   \n",
       "2     cellular   16   apr       185         1    330         1   \n",
       "3  unspecified    3   jun       199         4     -1         0   \n",
       "4  unspecified    5   may       226         1     -1         0   \n",
       "\n",
       "          poutcome   y  \n",
       "0  unknown_outcome  no  \n",
       "1          failure  no  \n",
       "2          failure  no  \n",
       "3  unknown_outcome  no  \n",
       "4  unknown_outcome  no  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dc78058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cleaned Data ---\n",
      "\n",
      "Dataset shape: (4521, 17)\n",
      "\n",
      "--- Missing Values Check ---\n",
      "\n",
      "age          0\n",
      "job          0\n",
      "marital      0\n",
      "education    0\n",
      "default      0\n",
      "balance      0\n",
      "housing      0\n",
      "loan         0\n",
      "contact      0\n",
      "day          0\n",
      "month        0\n",
      "duration     0\n",
      "campaign     0\n",
      "pdays        0\n",
      "previous     0\n",
      "poutcome     0\n",
      "y            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# verify loaded data\n",
    "print(\"--- Cleaned Data ---\\n\")\n",
    "print(f\"Dataset shape: {df.shape}\\n\")\n",
    "print(\"--- Missing Values Check ---\\n\") \n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa170c",
   "metadata": {},
   "source": [
    "*The dataset contains no missing values with 4,521 observations and 17 variables (16 input variables and 1 output variable)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0d8580",
   "metadata": {},
   "source": [
    "### **Feature Engineering**\n",
    "\n",
    "The feature engineering process involves three primary steps:\n",
    "1. **Feature removal:** several columns will be removed before training the model to improve performance:\n",
    "\n",
    "    - Data leakage features: The features identified as source of data leakage ('duration', 'campaign') will be dropped.\n",
    "    \n",
    "    - Low-impact variable: The 'day' variable will be excluded. As seasonality impact is already being analyzed by 'month' variable, the specific day is unlikely to provide a meaningful signal and may introduce more noise than a valuable relationship with the outcome.\n",
    "\n",
    "2. **Categorical variables conversion:** For binary categorical variables, we will use binary endocing to convert them into a numerical format (1s and 0s) that machine learning models can process.\n",
    "\n",
    "3. **Encoding categorical variables:** For multi-class categorical variables, we will apply one-hot encoding. This technique converts each feature into multiple new binary columns, preventing the model from assuming a false ordinal relationship between the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7464e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep original clean df intact\n",
    "df_model = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845264d1",
   "metadata": {},
   "source": [
    "**Feature Removal**\n",
    "\n",
    "*'duration', 'campaign', 'day'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55522035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Remaining Variables ---\n",
      "\n",
      "['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'month', 'pdays', 'previous', 'poutcome', 'y']\n"
     ]
    }
   ],
   "source": [
    "# drop variables (duration, campaign, day)\n",
    "exclude_cols = ['duration', 'campaign', 'day']\n",
    "df_model = df_model.drop(columns=exclude_cols)\n",
    "print(\"--- Remaining Variables ---\\n\")\n",
    "print(df_model.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4487fc4d",
   "metadata": {},
   "source": [
    "**Categorical Variables Conversion**\n",
    "\n",
    "*'y', 'default', 'housing', 'loan'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2dc4310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Binary Variables Mapped ---\n",
      "\n",
      "   y  default  housing  loan\n",
      "0  0        0        0     0\n",
      "1  0        0        1     1\n",
      "2  0        0        1     0\n",
      "3  0        0        1     1\n",
      "4  0        0        1     0\n"
     ]
    }
   ],
   "source": [
    "binary_values = {'yes': 1, 'no': 0}\n",
    "\n",
    "df_model['y'] = df_model['y'].map(binary_values)\n",
    "df_model['default'] = df_model['default'].map(binary_values)\n",
    "df_model['housing'] = df_model['housing'].map(binary_values)\n",
    "df_model['loan'] = df_model['loan'].map(binary_values)\n",
    "\n",
    "print(\"--- Binary Variables Mapped ---\\n\")\n",
    "print(df_model[['y','default','housing','loan']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc09edef",
   "metadata": {},
   "source": [
    "**Multi-Class Categorical Variables Encoding**\n",
    "\n",
    "*'job', 'marital', 'education', 'contact', 'month', 'poutcome'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fc1c733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Multi-Class Variables Encoded ---\n",
      "\n",
      "New dataset shape: (4521, 38)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>y</th>\n",
       "      <th>job_blue-collar</th>\n",
       "      <th>job_entrepreneur</th>\n",
       "      <th>...</th>\n",
       "      <th>month_jul</th>\n",
       "      <th>month_jun</th>\n",
       "      <th>month_mar</th>\n",
       "      <th>month_may</th>\n",
       "      <th>month_nov</th>\n",
       "      <th>month_oct</th>\n",
       "      <th>month_sep</th>\n",
       "      <th>poutcome_other</th>\n",
       "      <th>poutcome_success</th>\n",
       "      <th>poutcome_unknown_outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1787</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  default  balance  housing  loan  pdays  previous  y  job_blue-collar  \\\n",
       "0   30        0     1787        0     0     -1         0  0            False   \n",
       "\n",
       "   job_entrepreneur  ...  month_jul  month_jun  month_mar  month_may  \\\n",
       "0             False  ...      False      False      False      False   \n",
       "\n",
       "   month_nov  month_oct  month_sep  poutcome_other  poutcome_success  \\\n",
       "0      False       True      False           False             False   \n",
       "\n",
       "   poutcome_unknown_outcome  \n",
       "0                      True  \n",
       "\n",
       "[1 rows x 38 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_cols = ['job','marital','education','contact','month','poutcome']\n",
    "df_model = pd.get_dummies(data=df_model, columns=multi_cols, drop_first=True)\n",
    "\n",
    "print(\"--- Multi-Class Variables Encoded ---\\n\")\n",
    "print(\"New dataset shape:\", df_model.shape)\n",
    "df_model.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cbd530",
   "metadata": {},
   "source": [
    "This section has transformed cleaned dataset into a model-ready, fully numeric format.\n",
    "\n",
    "- Feature exclusion: 'Duration' and 'campaign' variables were droppted to prevent data leakage, and the 'day' variable was dropped to minimize data noise.\n",
    "\n",
    "- Binary encoding: The target variable 'y' and the binary features ('default', 'housing', 'loan') were converted from yes/no to 1/0.\n",
    "\n",
    "- One-hot encoding: The 6 multi-class categorical features were converted into numeric dummy variables.\n",
    "\n",
    "The final dataset is fully numeric and is ready to be split for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d94371",
   "metadata": {},
   "source": [
    "### **Data Split & Feature Scaling**\n",
    "\n",
    "Before training, the data will be prepared using a two-step process to ensure reliable model evaluation and performance.\n",
    "\n",
    "1. **Stratified data split:** The dataset will be split into an 80% training set and 20% test set. Because the dataset is imbalanced (approximately 11.5% positive class), we will use a **stratified split.** This ensures the proportion of subscribers is the same in both the training and test sets, which is a best practice for imbalanced classification problems.\n",
    "\n",
    "2. **Feature scaling:** After splitting the data, all numerical variables will be scaled. This step is neccessary to prevent numerical instability in distance-based algorithms like SMOTE and is performed after the split to avoid data leakage from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd0e1c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Splitting Verification ---\n",
      "\n",
      "Training set shape: (3616, 37)\n",
      "Testing set shape: (905, 37)\n",
      "\n",
      "Subscription rate in original dataset: 11.52%\n",
      "Subscription rate in training dataset: 11.53%\n",
      "Subscription rate in testing dataset: 11.49%\n"
     ]
    }
   ],
   "source": [
    "# 'y' is target variable\n",
    "# rest will be input variables\n",
    "y = df_model['y']\n",
    "X = df_model.drop(columns='y')\n",
    "# 80 training vs. 20 test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=12, stratify=y)\n",
    "\n",
    "# scaling numerical variables\n",
    "num_cols = ['age','balance','pdays','previous']\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(X_train[num_cols])\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[num_cols] = scaler.transform(X_train[num_cols])\n",
    "X_test_scaled[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "print(\"--- Data Splitting Verification ---\\n\")\n",
    "print(\"Training set shape:\", X_train_scaled.shape)\n",
    "print(\"Testing set shape:\", X_test_scaled.shape)\n",
    "\n",
    "print(\"\\nSubscription rate in original dataset:\", f\"{y.mean()*100:.2f}%\")\n",
    "print(\"Subscription rate in training dataset:\", f\"{y_train.mean()*100:.2f}%\")\n",
    "print(\"Subscription rate in testing dataset:\", f\"{y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab4ec6d",
   "metadata": {},
   "source": [
    "### **Class Imbalance Handling**\n",
    "\n",
    "To prevent model from ignoring the minority class due to the 90/10 imbalance, we will use SMOTE (synthetic minority over-sampling technique) on the training set to create a balanced dataset for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7746ac88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Class Imbalance Handling ---\n",
      "\n",
      "The original training dataset shape: (3616, 37)\n",
      "Subscription rate in original training dataset: 11.53%\n",
      "\n",
      "Resampled training dataset shape: (6398, 37)\n",
      "Subscription rate in resampled training dataset: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# smote to the training set\n",
    "smote = SMOTE(random_state=12)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(\"--- Class Imbalance Handling ---\\n\")\n",
    "print(\"The original training dataset shape:\", X_train_scaled.shape)\n",
    "print(\"Subscription rate in original training dataset:\", f\"{y_train.mean()*100:.2f}%\")\n",
    "print(\"\\nResampled training dataset shape:\", X_train_smote.shape)\n",
    "print(\"Subscription rate in resampled training dataset:\", f\"{y_train_smote.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c21be",
   "metadata": {},
   "source": [
    "### **Model Training & Evaluation**\n",
    "\n",
    "In this section, we will develop and evaluate several classification models to predict customer subscriptions. Our modeling strategy will proceed in three key steps:\n",
    "\n",
    "1. **Establish a baseline:** We will begin by building a simple, interpretable model to serve as a solid performance benchmark.\n",
    "\n",
    "2. **Develop advanced models:** We will train more complex models with the goal of outperforming the baseline.\n",
    "\n",
    "3. **Hyperparameter tuning:** Next, the best-perfoming model will be selected and its hyperparameters will be tuned to maximize its predictive power. \n",
    "\n",
    "The models to be trained are:\n",
    "- **Logistic regression:** This will serve as our baseline model. It's a highly interpretable and efficient model that provides a solid benchmark for comparison.\n",
    "\n",
    "- **Random forest and XGBoost:** These are industry-standard ensemble models, which are expected to offer higher predictive performance.\n",
    "\n",
    "Each model's performance will be assessed using **precision**, **recall**, **ROC AUC score**, and the **f1-score.** These are optimal metrics for an imbalanced dataset and will help us evaluate the trade-offs relevant to our business objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c17ec64",
   "metadata": {},
   "source": [
    "**-Baseline Logistic Regression Model-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f698d8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression Performance ---\n",
      "\n",
      "Accuracy: 0.786\n",
      "Precision: 0.204\n",
      "Recall: 0.298\n",
      "ROC AUC score: 0.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/young/opt/anaconda3/envs/bank_env/lib/python3.10/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/young/opt/anaconda3/envs/bank_env/lib/python3.10/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/young/opt/anaconda3/envs/bank_env/lib/python3.10/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/young/opt/anaconda3/envs/bank_env/lib/python3.10/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/young/opt/anaconda3/envs/bank_env/lib/python3.10/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/young/opt/anaconda3/envs/bank_env/lib/python3.10/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "log_reg = LogisticRegression(random_state=12, solver='liblinear')\n",
    "# train on the balanced training set\n",
    "log_reg.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# prediction on imbalanced test set\n",
    "pred_lr = log_reg.predict(X_test_scaled)\n",
    "# prediction probabilities for 'yes'\n",
    "prob_lr = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"--- Logistic Regression Performance ---\\n\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, pred_lr):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, pred_lr):.3f}\")\n",
    "print(f\"Recall: {recall_score(y_test, pred_lr):.3f}\")\n",
    "print(f\"ROC AUC score: {roc_auc_score(y_test, prob_lr):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1022f1be",
   "metadata": {},
   "source": [
    "*-Note on Runtime Warnings-*\n",
    "\n",
    "During training, the logistic regression model produced several RuntimeWarning messages, indicating that its optimization algorithm had difficulty converging, even with scaled data. While further tuning could be applied, we will accept the generated metrics as our preliminary baseline. The primary goal is to determine if more complex models provide a significant performance boost.\n",
    "\n",
    "We do not anticipate these warnings for random forest and XGBoost, as tree-based models are insensitive to feature scaling and do not rely on the same type of convergence-based optimization.\n",
    "\n",
    "*-Further Note-*\n",
    "\n",
    "To solve the issue, we moved from a standard scaler to a robust scaler and implemented outlier capping to handle extreme values. When the issue persisted, we switch to a 'RidgeClassifier' to handle potential multicollinearity. Since the warnings remained even with the above troubleshooting process, we concluded the issue was a fundamental incompatibility between the post-SMOTE data and linear model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fc6aff",
   "metadata": {},
   "source": [
    "**Baseline Model Evaluation**\n",
    "\n",
    "The initial logistic regression model serves as our baseline. The performance on the test set is as follows:\n",
    "\n",
    "- **Precision (0.204):** When the model predicts a client will subscribe, it is correct only 20.4% of the time. This means nearly 80% of marketing calls made based on these predictions would be to uninterested clients, highlighting significant inefficiency.\n",
    "\n",
    "- **Recall (0.298):** The model successfully identifies only 29.8% of all clients who actually would subscribe. This means over 70% of potential customers are being missed by this strategy.\n",
    "\n",
    "- **ROC AUC Score (0.613):** This score confirms that the model has learned some predictive patterns, performing slightly better than a random guess.\n",
    "\n",
    "The baseline model quantifies the business problem. A strategy based on this model would be inefficient (low precision) and would miss the majority of potential customers (low recall). Our goal now is to improve upon these metrics with more advanded models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c629c20",
   "metadata": {},
   "source": [
    "**-Random Forest Model-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7d931ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Random Forest Performance ---\n",
      "\n",
      "Accuracy: 0.851\n",
      "Precision: 0.322\n",
      "Recall: 0.269\n",
      "ROC AUC score: 0.690\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "rf_mod = RandomForestClassifier(random_state=12)\n",
    "# train on the balanced training set\n",
    "rf_mod.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "pred_rf = rf_mod.predict(X_test_scaled)\n",
    "prob_rf = rf_mod.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"--- Random Forest Performance ---\\n\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, pred_rf):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, pred_rf):.3f}\")\n",
    "print(f\"Recall: {recall_score(y_test, pred_rf):.3f}\")\n",
    "print(f\"ROC AUC score: {roc_auc_score(y_test, prob_rf):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e9b5e",
   "metadata": {},
   "source": [
    "**Random Forest Model Evaluation**\n",
    "\n",
    "The random forest model demonstrates a significant performance improvement over our baseline. Its performance on the test set is as follows:\n",
    "\n",
    "- **Precision (0.322):** When the model predicts a client will subscirbe, it is correct 32.2% of the time. This represents a major boost in marketing efficiency.\n",
    "\n",
    "- **Recall (0.269):** By being more selective, the model identified a slightly smaller portion of all potential subscribers (26.9%) compared to the baseline.\n",
    "\n",
    "- **ROC AUC Score (0.690):** This score indicates a strong ability to distinguish between subscribing and non-subscribing clients, performing much better than a random guess and surpassing the baseline. \n",
    "\n",
    "Overall, the random forest model is the superior model. The small drop is an acceptable trade-off for the significant gain in precision, leading to a much more efficient marketing outreach. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd114a3",
   "metadata": {},
   "source": [
    "**-XGBoost Model-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "158ea757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- XGBoost Performance ---\n",
      "\n",
      "Accuracy: 0.864\n",
      "Precision: 0.358\n",
      "Recall: 0.231\n",
      "ROC AUC score: 0.675\n"
     ]
    }
   ],
   "source": [
    "# xgboost\n",
    "xgb_mod = XGBClassifier(random_state=12)\n",
    "xgb_mod.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "pred_xgb = xgb_mod.predict(X_test_scaled)\n",
    "prob_xgb = xgb_mod.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"--- XGBoost Performance ---\\n\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, pred_xgb):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, pred_xgb):.3f}\")\n",
    "print(f\"Recall: {recall_score(y_test, pred_xgb):.3f}\")\n",
    "print(f\"ROC AUC score: {roc_auc_score(y_test, prob_xgb):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f2399a",
   "metadata": {},
   "source": [
    "**XGBoost Model Evaluation**\n",
    "\n",
    "The XGBoost model presents the clearest trade-off between precision and recall, prioritizing marketing efficiency above all else.\n",
    "\n",
    "- **Precision (0.358):** At 35.8%, this model is the most precise of the three, making it the most efficient in terms of targeted marketing spend.\n",
    "\n",
    "- **Recall (0.231):** As a result of being the most selective, it captures the smallest portion of all potential subscribers (23.1%).\n",
    "\n",
    "- **ROC AUC Score (0.675):** This score indicates a good ability to distinguish between classes; better than the baseline, but slightly lower than the random forest model.\n",
    "\n",
    "In summary, compared to the random forest, the XGBoost model achieves a marginal gain in precision by sacrificing a more significant amount of recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6255b41a",
   "metadata": {},
   "source": [
    "**Final Model Comparion & Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "514e6102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- F1 Score Comparison ---\n",
      "\n",
      "Logistic Regression\n",
      "F1 score: 0.242\n",
      "\n",
      "Random Forest\n",
      "F1 score: 0.293\n",
      "\n",
      "XGBoost\n",
      "F1 score: 0.281\n"
     ]
    }
   ],
   "source": [
    "print(\"--- F1 Score Comparison ---\\n\")\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"F1 score: {f1_score(y_test, pred_lr):.3f}\\n\")\n",
    "print(\"Random Forest\")\n",
    "print(f\"F1 score: {f1_score(y_test, pred_rf):.3f}\\n\")\n",
    "print(\"XGBoost\")\n",
    "print(f\"F1 score: {f1_score(y_test, pred_xgb):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bc7d66",
   "metadata": {},
   "source": [
    "A comparison of the F1 score, a metric that balances precision and recall, confirms that the random forest model (0.293) is the top performer. It surpasses both the XGBoost model (0.281) and the logistic regression baseline (0.242). \n",
    "\n",
    "Therefore, we will select the random forest model for the final phase of this section: hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6fc935",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e23622ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters:\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Best ROC AUC Score on Cross-Validation: 0.962\n"
     ]
    }
   ],
   "source": [
    "para_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=12),\n",
    "                           param_grid=para_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=3,\n",
    "                           verbose=0)\n",
    "\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(f\"\\nBest ROC AUC Score on Cross-Validation: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "best_rf_mod = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6592c1d",
   "metadata": {},
   "source": [
    "The optimal parameters found by the grid search were 200 estimators with no maximum depth. The tree-growing conditions require a minimum of 2 samples to split an internal node and a minimum of 1 sample per leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8c6edf",
   "metadata": {},
   "source": [
    "### **Identify Best Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb14b1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tuned Random Forest Performance ---\n",
      "\n",
      "Accuracy: 0.856\n",
      "Precision: 0.338\n",
      "Recall: 0.260\n",
      "F1 score: 0.293\n",
      "ROC AUC score: 0.687\n"
     ]
    }
   ],
   "source": [
    "pred_rf_tuned = best_rf_mod.predict(X_test_scaled)\n",
    "prob_rf_tuned = best_rf_mod.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"--- Tuned Random Forest Performance ---\\n\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, pred_rf_tuned):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, pred_rf_tuned):.3f}\")\n",
    "print(f\"Recall: {recall_score(y_test, pred_rf_tuned):.3f}\")\n",
    "print(f\"F1 score: {f1_score(y_test, pred_rf_tuned):.3f}\")\n",
    "print(f\"ROC AUC score: {roc_auc_score(y_test, prob_rf_tuned):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb19ba5",
   "metadata": {},
   "source": [
    "### **Final Model Selection & Conclusion**\n",
    "\n",
    "After comparing our three models, the random forest architecture was selected as the top performer based on its ROC AUC and F1-score. We then used 'GridSearchCV' to tune its hyperparameters to optimize its predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a0772",
   "metadata": {},
   "source": [
    "The tuning process resulted in a final model with the following performance on the test set:\n",
    "\n",
    "- **Precision: 0.338**\n",
    "- **Recall: 0.260**\n",
    "- **F1 Score: 0.293**\n",
    "- **ROC AUC: 0.687**\n",
    "\n",
    "Compared to the default settings, the tuned model became slightly more **precision-focused,** making it even more efficient at identifying high-probability leads with a minimal trade-ff in recall. We will proceed with this **Tuned Random Forest** as our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae99171",
   "metadata": {},
   "source": [
    "**Business Insight**\n",
    "\n",
    "The application of this model now depends on the bank's primary business objective, as its predictions can be used for two distinct strategies:\n",
    "\n",
    "- **Strategy 1: Maximize Market Reach (Prioritizing Recall)**\n",
    "    \n",
    "    - Use the model to identify the largest possible pool of potential subscribers, accepting a less efficient marketing spend.\n",
    "\n",
    "- **Strategy 2: Maximize Marketing ROI (Prioritizing Precision)**\n",
    "\n",
    "    - Use the model to target only the highest-probability leads, accepting that some potential customers will be missed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef5db02",
   "metadata": {},
   "source": [
    "**Next Steps**\n",
    "\n",
    "Our final notebook will analyze these strategic trade-offs in detail, explore the model's feature importances, and provide a definitive, data-driven recommendation for the bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ae6ae17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved\n",
      "Test data saved\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(best_rf_mod, '../models/tuned_rf_mod.joblib')\n",
    "joblib.dump(scaler, '../models/scaler.joblib')\n",
    "print(f\"Final model saved\")\n",
    "\n",
    "X_test.to_csv('../data/X_test_original.csv', index=False) # original, unscaled test data\n",
    "X_test_scaled.to_csv('../data/X_test_scaled.csv', index=False) # scaled test data\n",
    "y_test.to_csv('../data/y_test.csv', index=False) # untouched target test data\n",
    "print(\"Test data saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29fb687",
   "metadata": {},
   "source": [
    "*We save the trained model and the data to load it instantly in the next notebook (03_business_impact_and_conclusion)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bank_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
